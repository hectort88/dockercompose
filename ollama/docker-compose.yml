version: '3.9'

services:
  open-webui:
    image: backplane/open-webui:0-cuda # Replace with the actual image name
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "8080:8080" # Adjust the port as needed
    volumes:
      - ./webui-data:/data # Mount any necessary volumes
    environment:
      - CUDA_VISIBLE_DEVICES=0 # Adjust as needed for your GPU setup
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # Use all available GPUs

  ollama:
    image: ollama/ollama:latest # Replace with the actual image name
    container_name: ollama
    restart: unless-stopped
    ports:
      - "8081:8081" # Adjust the port as needed
    volumes:
      - ./ollama-data:/data # Mount any necessary volumes
    environment:
      - CUDA_VISIBLE_DEVICES=0 # Adjust as needed for your GPU setup
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # Use all available GPUs

networks:
  default:
    driver: bridge
